import random
import time
import threading

# Define text wrapping function for ease of reading
def print_wrapped(text, width=140):
    words = text.split()
    lines = []
    current_line = ''

    for word in words:
        if len(current_line) + len(word) <= width:
            current_line += word + ' '
        else:
            lines.append(current_line)
            current_line = word + ' '
    lines.append(current_line)

    for line in lines:
        print(line.strip())

# Define the test format and add scores
def run_test(questions):
    score = 0
    total_questions = len(questions)

# Setup a timer to simulate exam
    start_time = time.time()  # Start the timer
  
    for i, (question, options, answer) in enumerate(questions, 1):
        question_prompt = (f"\033[1mQuestion {i}:\033[0m {question}")
        print_wrapped(question_prompt)
        print("")
        
        #random.shuffle(options)

        for letter, option in zip("ABCDE", options):
            print(f"{letter}. {option}")
        
        user_answer = input("Your answer (A/B/C/D/E): ").strip().upper()
        
        if user_answer == answer:
            print("\033[1;38;5;34mCorrect!\033[0m\n\n")
            score += 1
        else:
            print(f"\033[1;31mIncorrect.\033[0m The correct answer is: \033[1m{answer}\033[0m\n\n")

        print("--------------------------------------------------------------------------------------------------------------------------------")

    end_time = time.time()  # End the timer
    elapsed_time = end_time - start_time
    print("\nTest Complete!")
    print(f"Your score: {score}/{total_questions}")
    print(f"Percentage: {(score/total_questions)*100}%")
    print(f"Time elapsed: {elapsed_time} seconds")

if __name__ == "__main__":

# Question bank in the form of tuples(question, options, answer)
    questions = [
        ("What is the main difference between AUTO LOADER and COPY INTO?", ["COPY INTO supports schema evolution", "AUTO LOADER supports schema evolution", "COPY INTO supports file notification when performing incremental loads", "AUTO LOADER supports reading data from Apache Kafka", "AUTO LOADER supports file notification when performing incremental loads"],  5),
        ("Which of the following locations in Databricks product architecture hosts jobs/pipelines and queries?", ["Data Plane", "Control Plane", "Databricks Filesystem", "JDBC data source", "Databricks Web Application"], 2),
        ("What type of table is created when you run the following query: CREATE TABLE transactions USING DELTA LOCATION ""DBFS:/mnt/bronze/transactions""?", ["Managed delta table", "External table", "Managed table", "Temp table", "Delta Lake table"], 2),
        ("You are working on a process to query the table based on batch date, and batch date is an input parameter, and expected to change every time the program runs.  What is the best way to parameterize the query to run without manually changing the batch date?", ["Create a notebook parameter for batch date and assign the value to a python variable and use a spark data frame to filter the data based on the python variable.", "Create a dynamic view that can calculate the batch date atuomatically and use the view to query the data.", "There is no way we can combine python variable and spark code.", "Manually edit code every time to change the batch date.","Store the batch date in the spark configuration and use a spark data frame to filter the data based on the spark configuration."], 1),
        ("When building a DLT's pipeline you have two options to create a live table.  What is the main difference between: CREATE STREAMING LIVE TABLE, and CREATE LIVE TABLE?", ["CREATE STREAMING LIVE table is used in multi-hop architecture", "CREATE LIVE TABLE is used when working with Streaming data sources and incremental data.", "CREATE STREAMING LIVE TABLE is used when working with streaming data sources and incremental data.", "There is no difference both are the same, CREATE STREAMING TABLE will be depracated soon.", "CREATE LIVE TABLE is used in delta live tables, CREATE STREAMING LIVE can only be used in structured streaming application."], 3),
        ("What are the different ways you can schedule a job in Databricks workspace?",["Continuous, Incremental", "On-Demand runs, File notification from cloud object storage", "Cron, On demand runs", "Cron, File notification from cloud object storage", "Once, Continuous"], 3),
        ("You have noticed that Databricks SQL queries are running slow.  You are asked to look into the reason why the queries are running slow, and identify steps to improve the performance.  When you looked at the issue you noticed all the queries are running in parallel and using a SQL endpoint(SQL Warehouse) with a single cluster.  Which of the following steps can be taken to improve the performance/response times of the queries?", ["They can turn on the Serverless feature for the SQL endpoint(SQL Warehouse)", "They can increase the maximum bound of the SQL endpoint(SQL Warehouse)'s scaling range.", "They can increase the warehouse size from 2X-Small to 4XLarge of the SQL endpoint(SQL Warehouse)", "They can turn on the Auto Stop feature for the SQL endpoint(SQL Warehouse)", "They can turn on the Serverless feature for the SQL endpoint(SQL Warehouse) and change the Spot Instance Policy to Reliability Optimized."], 2),
        ("You are currently working with the marketing team to setup a dashboard for ad campaign analysis.  Since the team is not sure how often the dashboard should be refreshed, they have decided to do a manual refresh on an as needed basis.  Which of the following steps can be taken to reduce the overall cost of the compute when the team is not using the compute?", ["They can turn on the Serverless feature for the SQL endpoint(SQL Warehouse)", "They can decrease the maximum bound of the SQL endpoint(SQL Warehouse) scaling range", "They can decrease the cluster size of the SQL endpoint(SQL Warehouse).", "They can turn on the Auto Stop feature for the SQL endpoint(SQL Warehouse).", "They can turn on the Serverless feature for the SQL endpoint(SQL Warehouse) and change the spot instance policy from Reliability Optimized to Cost Optimized"], 4),
        ("The research team has put together a funnel analysis query to monitor the customer traffic on the e-commerce platform.  The query takes about 30 minutes to run on a small SQL endpoint cluster with max scaling set to 1 cluster.  What steps can be taken to improve the performance of the query?", ["They can turn on the Serverless feature for the SQL endpoint.", "They can increase the maximum bound of the SQL endpoint's scaling range anywhere from between 1 to 100 to review the performance and select the size that meets the required SLA.", "They can increase the cluster size anywhere from X Small to 3XL to review the performance and select the size that meets the required SLA.", "They can turn off the Auto Stop feature for the SQL endpoint to more than 30 minutes.", "They can turn on the Serverless feature for the SQL endpoint and change the Spot instance policy from Cost Optimized to Reliability Optimized."], 3),
        ("Unity catalog simplifies managing multiple workspaces by storing and managing permissions and ACL at what level?", ["Workspace", "Account", "Storage", "Data plane", "Control Plane"], 2),
        ("Which of the following is not a privilege in the Unity catalog?", ["SELECT", "MODIFY", "DELETE", "CREATE TABLE", "EXECUTE"], 3),
        ("A team member is leaving and he/she is currently the owner of the few tables, instead of transferring the ownership to a user you have decided to transfer the ownership to a group. This way in the future, anyone in the group can manage the permissions rather than a single individual.  Which of the following commands help you accomplish this?", ["ALTER TABLE table_name OWNER to 'group'", "TRANSFER OWNER table_name to 'group'", "GRANT OWNER table_name to 'group'", "ALTER OWNER ON table_name to 'group'", "GRANT OWNER ON table_name to 'group'"], 1),
        ("You are currently working on a production job failure with a job setup in job clusters due to a data issue.  What cluster do you need to start to investigate and analyze the data?", ["A job cluster can be used to analyze the problem.", "All-purpose cluster/interactive cluster is the recommended way to run commands and view the data.", "Existing job cluster can be used to investigate the issue.", "Databricks SQL Endpoint can be used to investigate the issue.", "Databricks workflows logs can be used to investigate the issue"], 2),
        ("Which of the following is a correct statement on how the data is organized in the storage when managing a DELTA table?", ["All of the data is broken down into one or many parquet files, log files are broken down into one or many JSON files, and each transaction creates a new data file(s) and log file", "All of the data and log are stored in a single parquet file", "All of the data is broken down into one or many parquet files, but the log file is stored as a single JSON file, and every transaction creates a new data file(s) and log file gets apprended.", "All of the data is broken down into one or many parquet files, log file is removed once the transaction is committed.", "All of the data is stored into one parquet file, log files are broken down into one or many JSON files."], 1),
        ("What is the purpose of a silver layer in a multi-hop architecture?", ["Replaces a traditional data lake", "Efficient storage and querying of full and unprocessed history of data.", "A schema is enforced, with data quality checks.", "Refined views with aggregated data.", "Optimized query performance for business-critical data."], 3),
        ("You are currently asked to work on building a data pipeline. You have noticed that you are currently working on a very large scale ETL with many data dependencies.  Which of the following tools can be used to address this problem?", ["AUTO LOADER", "Jobs and Tasks", "SQL Endpoints", "DELTA LIVE TABLES", "Structured Streaming with Multi-Hop"], 4),
        ("You are noticing a job cluster that is taking 6 to 8 minutes to start which is delaying your job to finish on time.  What steps can you take to reduce the amount of time it takes for the cluster to start?", ["Setup a second job ahead of first job to start the cluster, so the cluster is ready with resources when the job starts.", "Use an All Purpose cluster to reduce cluster start up time.", "Reduce the size of the cluster, smaller the cluster size the shorter it takes to start.", "Use cluster pools to reduce the startup time of the jobs.", "Use SQL endpoints to reduce the startup time."], 4),
        ("Which of the following programming languages can be used to build a Databricks SQL dashboard?", ["Python", "Scala", "SQL", "R", "All of the above"], 3),
        ("The marketing team is launching a new campaign to monitor the performance of the new campaign for the first two weeks. They would like to setup a dashboard with a refresh schedule to run every 5 minutes. Which of the below steps can be taken to reduce the cost of this refresh over time?", ["Reduce the size of the SQL cluster size", "Reduce the max size of auto scaling from 10 to 5", "Setup the dashboard refresh schedule to end in two weeks.", "Change the spot instance policy from reliability optimized to cost optimized.", "Always use X-Small cluster"], 3),
        ("Which of the following tools provide data access control, access audit, data lineage, and data discovery?", ["Delta Live Pipelines", "Unity Catalog", "Data Governance", "Delta Lake", "Lakehouse"], 2),
        ("The Data Engineering team is required to share the data with the Data Science team.  Both of the teams are using different workspaces in the same organization.  Which of the following techniques can be used to simplify sharing data across the workspaces?", ["Data Sharing", "Unity catalog", "Delta lake", "Use a single storage location", "Delta Live pipelines"], 2),
        ("When you drop an external Delta table using the SQL command 'DROP TABLE table_name', how does it impact metadata, and data stored in the storage?", ["Drops table from metastore, metadata, and data in storage", "Drops table from metastore, but keeps metadata in storage", "Drops table from metastore, metadata, but it keeps the data in storage", "Drops table from metastore, but keeps metadate, and data in storage", "Drops table from metastore and data in storage but keeps metadata."], 4),
        ("Which of the following techniques does structured streaming use to ensure recovery of failures during stream processing?", ["Checkpointing and Watermarking", "Write Ahead logging and Watermarking", "Checkpointing and Write Ahead logging", "Delta time travel", "The stream will failover to available nodes in the cluster", "Checkpointing and Idempotent sinks"], 3),
        ("Which of the statements are correct about lakehouse?", ["Lakehouse only supports Machine Learning workloads and Data Warehouses support BI workloads", "Lakehouse only supports end-to-end streaming workloads and Data Warehouses support Batch workloads.", "Lakehouse does not support ACID.", "In Lakehouse, storage and compute are coupled.", "Lakehouse supports schema enforcement and evolution"], 5),
        ("Which of the following are stored in the control plane of Databricks architecture?",["Job Clusters", "All Purpose Clusters", "Databricks Filesystem", "Databricks Web Application", "Delta Tables"], 4),
        ("Which of the following developer operations in the CI/CD can only be implemented through a GIT provider when using Databricks repos?", ["Trigger Databricks repos pull API to update the latest version.", "Commit and push code.", "Create and edit code.", "Create a new branch.", "Pull request and review process."], 5),
        ("You have noticed the Data Scientist team is using the notebook versioning feature with GIT integration. You have recommended them to switch to using Databricks repos. Which of the below reasons could be the reason why the team needs to switch to Databricks repos?", ["Databricks repos allows multiple users to make changes.", "Databricks repos allows merge and conflict resolution.", "Databricks repos has a built-in version control system.", "Databricks repos automatically saves changes.", "Databricks repos allow you to add comments and select the changes you want to commit."], 5),
        ("Data Science team members are using a single cluster to perform data analysis.  Although the cluster size was chosen to handle multiple users and auto-scaling was enabled, the team realized queries are still running slow. What would be the suggested fix for this?", ["Setup multiple clusters so each team member has their own cluster.", "Disable the auto-scaling feature.", "Use high concurrency mode instead of the standard mode.", "Increase the size of the driver node.", "Increase the cluster size to max."], 3),
        ("How are Delta tables stored?", ["A directory where Parquet data files are stored, a sub directory_delta_log where metadata, and the transaction log is stored as JSON files.", "A directory where Parquet data files are stored, all of the metadata is stored in memory.", "A directory where Parquet data files are stored in the Data Plane, a sub directory_delta_log where metadata, history and log is stored in Control plane.", "A directory where Parquet data files are stored, all of the metadata is stored in Parquet files.", "Data is stored in the Data Plane and metadata and delta log are stored in the Control Plane."], 1),
        ("You want to drop the customers database and all of the associated tables/data. All of the tables inside the database are managed tables. Which of the following SQL commands will help you accomplish this?", ["DROP DATABASE customers FORCE", "DROP DATABASE customers CASCACDE", "DROP DATABASE customers INCLUDE", "All of the tables must be dropped first before dropping the database.", "DROP DELTA DATABASE customers"], 2),
        ("Which of the following table constraints can be enforced on Delta Lake tables?", ["Primary key, foreign key, NOT NULL, Check Constraints", "Primary Key, NOT NULL, Check Constraints", "Default, NOT NULL, Check Constraints", "NOT NULL, Check Constraints", "Unique, NOT NULL, Check Constraints"], 4),
        ("When writing streaming data, Spark's structured stream supports the below write modes?", ["Append, Delta, Complete", "Delta, Complete, Continuous", "Append, Complete, Update", "Complete, Incremental, Update", "Append, Overwrite, Continuous"], 3),
        ("The Data Engineering team noticed that one of the jobs fails randomly as a result of using spot instances. What feature in jobs/tasks can be used to address this issue so the job is more stable when using spot instances?", ["Use Databricks REST API to monitor and restart the job.", "Use Job runs, active runs UI section to monitor and restart the job.", "Add second task and add a check condition to rerun the first task if it fails.", "Restart the job cluster, job automatically restarts.", "Add a retry policy to the task."], 5),
        ("Why does AUTO LOADER require schema location?", ["Schema location is used to store user provided schema.", "Schema location is used to identify the schema of target table.", "AUTO LOADER does not require schema location, because it supports schema evolution.", "Schema location is used to store schema inferred by AUTO LOADER.", "Schema location is used to identify the schema of target table and source table."], 4),
        ("Which of the following statements are incorrect about the lakehouse?", ["Supports end-to-end streaming and batch workloads.", "Supports ACID", "Support for diverse data types that can store both structured and unstructured.", "Supports BI and Machine Learning", "Storage is coupled with compute."], 5),
        ("You are designing a data model that works for both machine learning using images and batch ETL/ELT workloads. Which of the following features of data lakehouse can help you meet the needs of both workloads?", ["Data lakehouse requires very little data modeling.", "Data lakehouse combines compute and storage for simple governance.", "Data lakehouse provides autoscaling for compute clusters.", "Data lakehouse can store unstructured data and support ACID transactions.", "Data lakehouse fully exists in the cloud."], 4),
        ("You are currently working on a notebook that will populate a reporting table for downstream process consumption. This process needs to run on a schedule every hour. What type of cluster are you going to use to set up this job?", ["Since it's just a single job and we need to run every hour, we can use an all-purpose cluster.", "The job cluster is best suited for this purpose.", "Using an Azure VM to read/write delta tables in Python.", "Use delta live table pipeline to run in continuous mode.", "Use the scheduling feature in the notebook to execute on a set schedule."], 2),
        ("Which of the following developer operations in CI/CD flow can be implemented in Databricks Repos?", ["Merge when code is committed.", "Pull request and review process.", "Trigger Databricks Repos API to pull the latest version of code into production folder.", "Resolve merge conflicts.", "Delete a branch."], 3),
        ("You are currently working with the second team and both teams are looking to modify the same notebook. You noticed that the second member is copying the notebooks to the personal folder to edit and replace the collaboration notebook. Which notebook feature do you recommend to make the process easier to collaborate?", ["Databricks notebooks should be copied to a local machine and setup source control locally to version the notebooks.", "Databricks notebooks support automatic change tracking and versioning.", "Databricks notebooks support real-time coauthoring on a single notebook.", "Databricks notebooks can be exported into dbc archive files and stored in data lake.", "Databricks notebook can be exported as HTML and imported at a later time."], 3),
        ("You are currently working on a project that requires the use of SQL and Python in a given notebook, what would be your approach?", ["Create two separate notebooks, one for SQL and the second for Python.", "A single notebook can support multiple languages, use the magic command to switch between the two.", "Use an all-purpose cluster for Python, SQL Endpoint for SQL.", "Use job cluster to run Python and SQL Endpoint for SQL.", "You can't use Python and SQL interchangeably in the same notebook."], 2),
        ("Which of the following statements are correct on how Delta Lake implements a lake house?", ["Delta lake uses a proprietary format to write data, optimized for cloud storage.", "Using Apache Hadoop on cloud object storage.", "Delta lake always stores metadata in memory vs storage.", "Delta lake uses open source, open format, optimized cloud storage and scalable metadata.", "Delta lake stores data and metadata in computes memory."], 4),
        ("If you run the command 'VACUUM transactions RETAIN 0 hours', what would the outcome of be?", ["Command will be successful, but no data removed.", "Command will fail if you have an active transaction running.", "Command will fail.  You cannot run the command wiht retentionDurationcheck enabled.", "Command will be successful, but historical data will be removed.", "Command runs successful and compacts all of the data in the table."], 3),
        ("You noticed a colleague is manually copying the data to the backup folder prior to running an update command. In case the update command did not provide the expected outcome so he can use the backup copy to replace table, which Delta Lake feature would you recommend simplifying the process?", ["Use the time travel feature to refer to old data instead of manually copying.", "Use DEEP CLONE to clone the table prior to update to make a backup copy.", "Use SHADOW copy of the table as the preferred backup choice.", "Cloud object storage retains previous version of the file.", "Cloud object storage automatically backups the data."], 1),
        ("Which of the following is not a Databricks lakehouse object?", ["Tables", "Views", "Database/Schemas", "Catalog", "Stored Procedures"], 5),
        ("Which of the following commands can be used to drop a managed delta table and the underlying files in the storage?", ["DROP TABLE table_name CASCADE", "DROP TABLE table_name", "Use DROP TABLE table_name command and manually delete files using command dbutils.fs.rm('/path',TRUE)", "DROP TABLE table_name INCLUDE_FILES", "DROP TABLE table_name and run VACUUM command"], 2),
        ("Which of the following is the correct statement for a session scoped temporary view?", ["Temporary views are lost once the notebook is detached and re-attached.", "Temporary views are stored in memory.",  "Temporary views can be still accessed even if the notebook is detached and attached.", "Temporary views can be still accessed even if the cluster is restarted.", "Temporary views are created in local_temp database"], 1),        ("Which of the following is correct for the global temporary view?", ["Global temporary views cannot be accessed once the notebook is detached and attached.", "Global temporary views can be accessed across many clusters.", "Global temporary views can be still accessed even if the notebook is detached and atached.", "Global temporary views can be still accessed even if the cluster is restarted.", "Global temporary views are created in a database called temp database."], 3),
        ("Which of the following SQL statements can be used to query a table by eliminating duplicate rows from the query results?", ["SELECT DISTINCT * FROM table_name", "Select DISTINCT * FROM table_name HAVING COUNT(*) > 1", "SELECT DISTINCT_ROWS(*) FROM table_name", "SELECT * FROM table_name GROUP BY * HAVING COUNT(*) < 1", "SELECT * FROM table_name GROUP BY * HAVING COUNT(*) > 1"], 1),
        ("Which of the following techniques does structured streaming use to create an end-to-end fault tolerance?", ["Checkpointing and Water Marking.", "Write Ahead logging and Water Marking.", "Checkpointing and idempotent sinks.", "Write Ahead logging and idempotent sinks.", "Stream will failover to available nodes in the cluster."], 3),
        ("Which of the following two options are supported in identifying the arrival of new files, and incremental data from Cloud object storage using AUTO LOADER?", ["Directory listing, File notification", "Checkpointing, Watermarking", "Write ahead logging, read ahead logging", "File hashing, Dynamic file lookup", "Checkpointing and write ahead logging"], 1),
        ("Which of the following data workloads will utilize a Bronze table as its destination?", ["A job that aggregates cleaned data to create standard summary statistics.", "A job that queries aggregated data to publish key insights into a dashboard.", "A job that ingests raw data from a streaming source into the Lakehouse.", "A job that develops a feature set for a machine learning application.", "A job that enriches data by parsing it's timestamps into a human-readable format."], 3),
        ("Which of the following data workloads will utilize a silver table as its source?", ["A job that enriches data by parsing it's timestamps into a human-readable format.", "A job that queries aggregated data that already feeds into a dashboard.", "A job that ingets raw data from a streaming source into the Lakehouse.", "A job that aggregates cleaned data to create standard summary statistics.", "A job that cleans data by removing malformatted records."], 4),
        ("Which of the following data workloads will utilize a gold table as its source?", ["A job that cleans data by removing malformatted records.", "A job that enriches data by parsing it's timestamps into a human-readable format.", "A job that queries aggregated data that already feeds into a dashboard.", "A job that ingets raw data from a streaming source into the Lakehouse.", "A job that aggregates cleaned data to create standard summary statistics."], 3),
        ("You are currently asked to work on building a data pipeline. You have noticed that you are currently working with a data source that has a lot of data quality issues, and you need to monitor data quality and enforce it as part of the data ingestion process. Which of the following tools can be used to address this problem?", ["AUTO LOADER", "Delta Live Tables", "Jobs, and Tasks", "Unity Catalog and Data Governance", "Structured Streaming with Multi-Hop"], 2),
        ("When building a DLT pipeline you have two options to create a live table. What is the main difference between CREATE STREAMING LIVE TABLE vs CREATE LIVE TABLE?", ["CREATE STREAMING LIVE table is used in Multi-Hop architecture.", "CREATE LIVE TABLE is used when working with streaming data sources and incremental data.", "CREATE STREAMING LIVE TABLE is used when working with streaming data sources and incremental data.", "There is no difference. Both are the same, CREATE STREAMING LIVE will be deprecated soon.", "CREATE LIVE TABLE is used in delta live tables, CREATE STREAMING LIVE can only be used in structured streaming applications."], 3),
        ("A particular job seems to be performing slower and slower over time, the team thinks this started to happen when a recent production change was implemented. You were asked to take look at the job history and see if you can identify trends and root cause. Where in the workspace UI can you perform this analysis?", ["Under the jobs UI select the job you are interested in, under runs you can see current active runs and the last 60 days of historical runs.", "Under the jobs UI select the job cluster, under the spark UI select the application job logs, then you can access the last 60 days of historical runs.", "Under workspace logs, select job logs and select the job you want to monitor to view the last 60 days of historical runs.", "Under compute UI, select job cluster and select the job cluster to see the last 60 days of historical runs.", "Historical job runs can only be accessed by the REST API"], 1),
        ("You had worked with the Data analysts team to set up a SQL Endpoint(SQL warehouse) point so they can easily query and analyze data in the gold layer, but once they started consuming the SQL Endpoint(SQL warehouse)  you noticed that during the peak hours as the number of users increase you are seeing queries take longer to finish. Which of the following steps can be taken to resolve the issue?", ["They can turn on the Serverless feature for the SQL endpoint(SQL Warehouse).", "They can increase the maximum bound of the SQL endpoint(SQL Warehouse)'s scaling range.", "They can increase the cluster size from 2X-Small to 4X-Large of the SQL endpoint(SQL Warehouse).", "They can turn on the Auto Stop feature for the SQL endpoint(SQL Warehouse).", "They can turn on the serverless feature for the SQL endpoint(SQL Warehouse), and change the spot instance policy from 'cost optimized' to 'reliability optimized'."], 2),
        ("Which of the following sections in the UI can be used to manage permissions and grants to tables?", ["User Settings", "Admin UI", "Workspace Admin Settings", "User Access Control Lists(ACL)", "Data Explorer"], 5),
        ("Which is the best way to describe a data lakehouse compared to a data warehouse?", ["A data lakehouse provides a relational system of data management.", "A data lakehouse captures snapshots of data for version control purposes.", "A data lakehouse couples storage and compute for complete control.", "A data lakehouse utilizes propietary storage formats for data.", "A data lakehouse enables both batch and streaming analytics."], 5),
        ("You are designing an analytical platform to store structured data from your e-commece platform and unstructured data from website traffic and app store. How would you approach where you store this data?", ["Use traditional data warehouse for structured data and use data lakehouse for unstructured data.", "Data lakehouse can only store unstructured data but cannot enforce a schema.", "Data lakehouse can store structured and unstructured data and can enforce schema.", "Traditional data warehouses are good for storing structured data and enforcing schema.", "Data warehouses cannot store unstructured data."], 3),
        ("Which of the following describes how Databricks repos can help facilitate CI/CD workflows on the Databricks Lakehouse platform?", ["Databricks repos can facilitate the pull request, review, and approval process before merging branches.", "Databricks repos can merge changes from a secondary GIT branch into a main GIT branch.", "Databricks Repos can be used to design, develop, and trigger GIT automation pipelines.", "Databricks Repos can store the single source of truth GIT repository.", "Databricks Repos can commit or push code changes to trigger a CI/CD process."],5),
        ("You noticed that a colleague is manually copying the notebook with _bkp to store the previous versions. Which of the following feature would you recommend instead?", ["Databricks notebooks support change tracking and versioning.", "Databricks notebooks should be copied to a local machine and setup source control locally to version the notebooks.", "Databricks notebooks can be exported into dbc archive files and stored in data lake.", "Databricks notebooks can be exported as HTML and impored at a later time.", "Databricks notebooks can be uploaded to Databricks communtiy cloud to store for backups."], 1),
        ("Newly joined data analyst requested read-only access to tables. Assuming you are owner/admin, which section of the Databricks platform is going to facilitate granting select access to the user?", ["Admin console", "User settings", "Data explorer", "Azure Databricks control pane IAM", "Azure RBAC"], 3),
        ("How does a delta lake differ from a traditional data lake?", ["Delta lake is a data warehouse service on top of data lake that can provide reliability, security, and performance.", "Delta lake is a caching layer on top of data lake that can provide reliability, security, and performance.", "Delta lake is an open storage format like parquet with additional capabilities that can provide reliability, security, and performance.", "Delta lake is an open storage format designed to replace flat files with additional capabilities that can provide reliability, security, and performance.", "Delta lake is proprietary software designed by Databricks that can provide reliability, security, and performance."], 3),
        ("What is the underlying technology that makes the AUTO LOADER work?", ["Loader", "Delta live tables", "Structured Streaming", "Dataframes", "Live Dataframes"], 3),
        ("You are currently working to ingest millions of files that get uploaded to the cloud object storage for consumption. You are asked to build a process to ingest this data. The schema of the file is expected to change over time, and the ingestion process should be able to handle these changes automatically. Which of the following methods can be used to ingest the data incrementally?", ["Auto Append", "AUTO LOADER", "Copy Into", "Structured Streaming", "Checkpoint"], 2),
        ("What is the purpose of a bronze layer in a multi-hop architecture?", ["Can be used to eliminate duplicate records.", "Used as a data source for machine learning applications.", "Perform data quality checks, corrupt data quarantined.", "Contains aggregated data that is to be consumed into Silver.", "Provides efficient storage and querying of full unprocessed history of data."], 5),
        ("What is the purpose of a gold layer in a multi-hop architecure?", ["Optimizzes ETL throughput and analytic query performance.", "Eliminate duplicate records.", "Preserves grain of original data, without any aggregations.", "Data quality checks and schema enforcement.", "Powers ML applications, reporting, dashboards and adhoc reports."], 5),
        ("How do you create a delta live tables pipeline and deploy using DLT UI?", ["Within the workspace UI, click on workflows, select delta live tables and create a pipeline and select the notebook with DLT code.", "Under cluster UI, select spark UI and select structured streaming and click create pipeline, then select the notebook with DLT code.", "There is no UI, you can only setup delta live tables using Python and SQL API and select the notebook with DLT code.", "Use VS Code and download DBX plugin, once the plugin is loaded you can build DLT pipelines and select the notebook with DLT code.", "Within the workspace UI, click on SQL Endpoint, select delta live tables and create pipelines and select the notebook with DLT code."], 1),
        ("Data engineering team has a job currently setup to run a task load data into a reporting table every day at 0800 that takes around 20 minutes. Operations teams are planning to use that data to run a second job, so that they have access to the latest complete set of data. What is they best way to orchestrate this job setup?", ["Add Operation reporting task in the same job and set the data engineering task to depend on operations reporting task.", "Setup a second job to run at 0820 in the same workspace.", "Add operation reporting task in the same job and set the operations reporting task to depend on data engineering task.", "Use AUTO LOADER to run every 20 minutes to read the initial table and set the trigger to once and create a second job.", "Setup a delta live to table based on the first table, set the job to run in continuous mode."], 3),
        ("The data engineering team noticed that one of the job normally finishes in 15 minutes but gets stuck randomly when reading remote databases due to a network packet drop. Which of the following steps can be used to improve the stability of the job?", ["Use Databricks REST API to monitor long running jobs and issue a kill command.", "Use job runs, active runs UI section to monitor and kill the long running job", "Modify the task, to include a timeout to kill the job if it runs more than 15 minutes.", "Use Spark job timeout setting in the Spark UI.", "Use cluster timeout setting in the job cluster UI"], 3),
        ("A newly joined team member John Smith in the Marketing team who currently does not have any access to the data requires read access to the customers table. Which of the following statements can be used to grant access?", ["GRANT SELECT, USAGE TO john.smith@marketing.com ON TABLE customers", "GRANT READ, USAGE TO john.smith@marketing.com ON TABLE customers", "GRANT SELECT, USAGE ON TABLE customers TO john.smith@marketing.com", "GRANT READ, USAGE ON TABLE customers TO john.smith@marketing.com", "GRANT READ, USAGE ON customers TO john.smith@marketing.com"], 3),
        ("Grant full priveleges to new marketing user Kevin Smith to table sales?", ["GRANT FULL PRIVELEGES TO kevin.smith@marketing.com ON TABLE sales", "GRANT ALL PRIVELEGES TO kevin.smith@marketing.com ON TABLE sales", "GRANT FULL PRIVELEGES ON TABLE sales TO kevin.smith@marketing.com", "GRANT ALL PRIVILEGES ON TABLE sales TO kevin.smith@marketing.com", "GRANT ANY PRIVILEGE ON TABLE sales TO kevin.smith@marketing.com"], 4),
        ("Which of the following locations in the Databricks product architecture hosts the notebooks and jobs?", ["Data plane", "Control plane", "Databricks filesystem", "JDBC data source", "Databricks web application"], 2),
        ("A dataset has been defined using Delta live tables and includes an expectations clause: CONSTRAINT valid_timestamp EXPECT(timestamp > '2020-01-01') ON VIOLATION FAIL. What is the expected behavior when a batch of data containing data that violates these constraints is processed?", ["Records that violate the expectation are added to the target dataset and recorded as invalid in the event log.", "Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.", "Records that violate the expectation cause the job to fail.", "Records that violate the excpectation are added to the target dataset and flagged as invalid in a field added to the target dataset.", "Records that violate the expectation are dropped from the target dataset and loaded into a quarantine table."], 3),
        ("You are still noticing slowness in query after performing optimize which helped you to resolve the small files problem.  The column(transactionID) you are using to filter the data has high cardinality and auto incrementing number. Which delta optimization can you enable to filter data effectively based on this column?", ["Create bloom filter index on the transactionid", "Perform optimize with Zorder on transactionID", "transactionID has high cardinality, you cannot enable any optimization", "Increase the cluster size and enable delta optimization", "Increase the driver size and enable delta optimization"], 2),
        ("If you create a database sample_db with the statement CREATE DATABASE sample_db, what will be the default location of the database in DBFS?", ["Default location, DBFS:/user/", "Default location, /user/db/", "Default storage account", "Statement fails 'unable to create database without location'", "Default location, dbfs:/user/hive.warehouse"], 5),
        ("Which of the following results in the creation of an external table?", ["CREATE TABLE transactions (id int, desc string) USING DELTA LOCATION EXTERNAL", "CREATE TABLE transactions (id int, desc string)", "CREATE EXTERNAL TABLE transactions (id int, desc string)", "CREATE TABLE transactions (id int, desc string) TYPE EXTERNAL", "CREATE TABLE transactions (id int, desc string) LOCATION '/mnt/delta/transactions'"], 5),
        ("Which of the following is a true statement about the global temporary view?", ["A global temporary view is available only on the cluster it was created, when the cluster restarts global temporary view is automatically dropped.", "A global temporary view is available on all clusters for a given workspace.", "A global temporary view persists even if the cluster is restarted.", "A global temporary view is stored in a user database.", "A global temporary view is automatically dropped after 7 days."], 1),
        ("You are trying to create an object by joining two tables that make it accessible to data scientist's team.  You want to create this object so that it does not get dropped if the cluster restarts or if the notebook is detached/reattached.  What type of object are you trying to create?", ["Temporary view", "Global temporary view", "Global temporary view with cache option", "External view", "View"], 5),
        ("Which of the statements are incorrect when choosing between lakehouse and Data Warehouse?", ["Lakehouse can have special indexes and caching which are optimized for MAchine Learning.", "Lakehouse cannot serve low query latency with high reliability for BI workloads, only suitable for batch workloads.", "Lakehouse can be accessed through various API's including but not limited to Python/R/SQL.", "Traditional Data Warehouses have storage and compute are coupled.", "Lakehouse uses standard data formats like Parquet."], 2),
        ("You have written a notebook to generate a summary data set for reporting. Notebook was scheduled using the job cluster, but you realized it takes 8 minutes to start the cluster. What feature can be used to start the cluster in a timely fashion so your job can run immediately?", ["Setup an additional job to run ahead of the actual job so the cluster is running second job starts.", "Use the Databricks cluster pools feature to reduce the startup time.", "Use Databricks premium edition instead of Databricks standard edition.", "Pin the cluster in the cluster UI page so it is always available to the jobs.", "Disable auto termination so the cluster is always running."], 2),
        ("Which of the following SQL commands are used to append rows to an existing delta table?", ["APPEND INTO DELTA table_name", "APPEND INTO table_name", "COPY DELTA INTO table_name", "INSERT INTO table_name", "UPDATE table_name"], 4),
        ("While investigating a data issue in a delta table, you wanted to review logs to see when and who updated the table. What is the best way to review this data?", ["Review event logs in the workspace", "Run SQL SHOW HISTORY table_name", "Check Databricks SQL Audit Logs", "Run the SQL command DESCRIBE HISTORY table_name", "Review workspace audit logs"], 4),
        ("While investigating a performance issue, you realized that you have too many small files for a given table. Which command are you going to run to fix this issue?", ["COMPACT table_name", "VACUUM table_name", "MERGE table_name", "SHRINK table_name", "OPTIMIZE table_name"], 5),
        ("What is the type of table created when you issue SQL DDL command CREATE TABLE sales (id int, units int)?", ["Query fails due to missing location.", "Query fails due to missing format.", "Managed Delta Table", "External Table", "Managed Parquet table"], 3)

    ]


# Shuffle the questions
random.shuffle(questions)
    
# Select a random subset of 45 questions
random_questions = random.sample(questions, 45)

#----------add
# Mapping of numeric positions to letters
position_to_letter = {1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E'}

# Create a list to store updated questions
updated_questions = []

# Shuffle options and update correct answer position for each question
for question, options, correct_answer_position in random_questions:
    # Shuffle options
    shuffled_options = random.sample(options, len(options))
    
    # Find the new position of the correct answer
    new_correct_answer_position = shuffled_options.index(options[correct_answer_position - 1]) + 1
    
    # Convert new correct answer position to letter
    new_correct_answer_letter = position_to_letter[new_correct_answer_position]
    
    # Append updated question to the list
    updated_questions.append((question, shuffled_options, new_correct_answer_letter))
#----END------add

# Set the timer for 90 minutes and print message when time expires
timer_thread = threading.Timer(90*60, lambda: print("Exam time has ended!"))
timer_thread.start()

# Run the test exam
run_test(updated_questions)

# Cancel the timer thread if the test is completed before time's up
timer_thread.cancel()
